{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYjg94ZLiaH3",
        "outputId": "de768cff-11c4-420f-8433-a50e3d155ca8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.4.1-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from gradio) (1.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from gradio) (2.11.3)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.18.3-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.8 MB/s \n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "Collecting websockets\n",
            "  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 55.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from gradio) (1.3.5)\n",
            "Collecting paramiko\n",
            "  Downloading paramiko-2.11.0-py2.py3-none-any.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 46.8 MB/s \n",
            "\u001b[?25hCollecting h11<0.13,>=0.11\n",
            "  Downloading h11-0.12.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from gradio) (1.21.6)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 49.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from gradio) (3.8.3)\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.7/dist-packages (from gradio) (2022.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from gradio) (6.0)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gradio) (2.23.0)\n",
            "Collecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.85.0-py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting markdown-it-py[linkify,plugins]\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.8 MB/s \n",
            "\u001b[?25hCollecting pycryptodome\n",
            "  Downloading pycryptodome-3.15.0-cp35-abi3-manylinux2010_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 37.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (0.13.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (1.8.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (4.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (6.0.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->gradio) (22.1.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp->gradio) (2.10)\n",
            "Collecting starlette==0.20.4\n",
            "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
            "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 8.3 MB/s \n",
            "\u001b[?25hCollecting sniffio>=1.1\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx->gradio) (2022.9.24)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.16.0,>=0.15.0\n",
            "  Downloading httpcore-0.15.0-py3-none-any.whl (68 kB)\n",
            "\u001b[K     |████████████████████████████████| 68 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->gradio) (2.0.1)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.1-py3-none-any.whl (46 kB)\n",
            "\u001b[K     |████████████████████████████████| 46 kB 3.5 MB/s \n",
            "\u001b[?25hCollecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->gradio) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->gradio) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->gradio) (2022.4)\n",
            "Collecting pynacl>=1.0.1\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[K     |████████████████████████████████| 856 kB 72.7 MB/s \n",
            "\u001b[?25hCollecting bcrypt>=3.1.3\n",
            "  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 40.6 MB/s \n",
            "\u001b[?25hCollecting cryptography>=2.5\n",
            "  Downloading cryptography-38.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.5->paramiko->gradio) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.5->paramiko->gradio) (2.21)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gradio) (3.0.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4712 sha256=3bee7f764035fc82334a2cacfc5eba68ffed5e0af4190ff1d626eb9e9eb27107\n",
            "  Stored in directory: /root/.cache/pip/wheels/13/e4/6c/e8059816e86796a597c6e6b0d4c880630f51a1fcfa0befd5e6\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=c0b08815b61f246321abc80f0d64af756fea6f288226786468d9a97285f79fe6\n",
            "  Stored in directory: /root/.cache/pip/wheels/2c/41/7c/bfd1c180534ffdcc0972f78c5758f89881602175d48a8bcd2c\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: sniffio, mdurl, uc-micro-py, rfc3986, markdown-it-py, h11, anyio, starlette, pynacl, mdit-py-plugins, linkify-it-py, httpcore, cryptography, bcrypt, websockets, uvicorn, python-multipart, pydub, pycryptodome, paramiko, orjson, httpx, ffmpy, fastapi, gradio\n",
            "Successfully installed anyio-3.6.1 bcrypt-4.0.1 cryptography-38.0.2 fastapi-0.85.0 ffmpy-0.3.0 gradio-3.4.1 h11-0.12.0 httpcore-0.15.0 httpx-0.23.0 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.1 mdurl-0.1.2 orjson-3.8.0 paramiko-2.11.0 pycryptodome-3.15.0 pydub-0.25.1 pynacl-1.5.0 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.20.4 uc-micro-py-1.0.1 uvicorn-0.18.3 websockets-10.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "juKldQhVh1Gr"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "from torchtext import data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "qpBKjJ21h1Gu"
      },
      "outputs": [],
      "source": [
        "class BaselineModel(torch.nn.Module):\n",
        "    def __init__(self, vocab):\n",
        "        super().__init__()\n",
        "        # initialize word vectors to random numbers \n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.out=torch.nn.Linear(100,1)\n",
        "        #TO DO\n",
        "        \n",
        "        # prediction function takes embedding as input, and predicts which word in vocabulary as output\n",
        "\n",
        "        #TO DO\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: torch.tensor of shape (bsz), bsz is the batch size\n",
        "        \"\"\"\n",
        "        #TO DO\n",
        "        e=0\n",
        "        for item in x:\n",
        "          e+=self.embedding(item)\n",
        "        e/= len(x)\n",
        "        logits=self.out(e).squeeze()\n",
        "        #out=torch.nn.functional.sigmoid(logits).squeeze()\n",
        "        \n",
        "        return logits\n",
        "\n",
        "# cnn model\n",
        "class CNNModel(torch.nn.Module): # CNN model\n",
        "    def __init__(self, vocab, n1,n2, k1,k2):\n",
        "        super().__init__()\n",
        "        # initialize word vectors to random numbers \n",
        "        self.embedding = torch.nn.Embedding.from_pretrained(vocab.vectors)\n",
        "        self.conv1=torch.nn.Conv2d(1,n1,(100,k1),bias=False)\n",
        "        self.conv2=torch.nn.Conv2d(1,n2,(100,k2),bias=False)\n",
        "        self.activate=torch.nn.ReLU()\n",
        "        self.maxpool= torch.nn.MaxPool2d((1,500),ceil_mode=True)\n",
        "        #self.maxpool2= torch.nn.MaxPool1d()\n",
        "        self.dropout = torch.nn.Dropout(0.25)\n",
        "        self.out= torch.nn.Linear(n1+n2,1)\n",
        "        #TO DO\n",
        "        \n",
        "        # prediction function takes embedding as input, and predicts which word in vocabulary as output\n",
        "\n",
        "        #TO DO\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: torch.tensor of shape (bsz), bsz is the batch size\n",
        "        \"\"\"\n",
        "        #TO DO\n",
        "        #print(x.size())\n",
        "        embed=self.embedding(x)\n",
        "        #print(embed.size())\n",
        "        embed=embed.T.unsqueeze(0)\n",
        "        #print(embed.size())\n",
        "        conv1=self.conv1(embed)\n",
        "        conv1=self.activate(conv1)\n",
        "        conv2=self.conv2(embed)\n",
        "        conv2=self.activate(conv2)\n",
        "        #conv1=conv1.squeeze()\n",
        "        #conv2=conv2.squeeze()\n",
        "        #print(conv1.size())\n",
        "        #print(conv2.size())\n",
        "        max1= self.maxpool(conv1)\n",
        "        #print(max1.size())\n",
        "        max2=self.maxpool(conv2)\n",
        "        #print(max2.size())\n",
        "        #drop=self.dropout(torch.cat((max1,max2)).squeeze())\n",
        "        #output=self.out(drop)\n",
        "        output=self.out(torch.cat((max1,max2)).squeeze())\n",
        "        return output\n",
        "\n",
        "glove = torchtext.vocab.GloVe(name=\"6B\",dim=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1yaw8KPh1Gv",
        "outputId": "41a52588-90a5-49ab-edfe-5d65dfb73f03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNModel(\n",
              "  (embedding): Embedding(400000, 100)\n",
              "  (conv1): Conv2d(1, 25, kernel_size=(100, 2), stride=(1, 1), bias=False)\n",
              "  (conv2): Conv2d(1, 25, kernel_size=(100, 3), stride=(1, 1), bias=False)\n",
              "  (activate): ReLU()\n",
              "  (maxpool): MaxPool2d(kernel_size=(1, 500), stride=(1, 500), padding=0, dilation=1, ceil_mode=True)\n",
              "  (dropout): Dropout(p=0.25, inplace=False)\n",
              "  (out): Linear(in_features=50, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# load baseline model from saved checkpoint\n",
        "baseline_model = BaselineModel(glove)\n",
        "baseline_model.load_state_dict(torch.load('/content/baseline.pt'))\n",
        "baseline_model.eval()\n",
        "\n",
        "# load cnn model from saved checkpoint\n",
        "cnn_model = CNNModel(glove,25,25,2,3)\n",
        "cnn_model.load_state_dict(torch.load('/content/CNN.pt', map_location=torch.device('cpu')))\n",
        "cnn_model.eval()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "T2Kyo7Sjh1Gw",
        "outputId": "adcb3fd1-b69e-4579-846a-d883bf83c47d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gradio/outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  \"Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\",\n",
            "/usr/local/lib/python3.7/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.7/dist-packages/gradio/interface.py:356: UserWarning: The `allow_flagging` parameter in `Interface` nowtakes a string value ('auto', 'manual', or 'never'), not a boolean. Setting parameter to: 'never'.\n",
            "  \"The `allow_flagging` parameter in `Interface` now\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://19556.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://19556.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<gradio.routes.App at 0x7f91e027f590>,\n",
              " 'http://127.0.0.1:7861/',\n",
              " 'https://19556.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# baseline prediction function with output probability and label\n",
        "def baseline_predict(sentence):\n",
        "\ttokens = sentence.split()\n",
        "\t# Convert to integer representation per token\n",
        "\ttoken_ints = [glove.stoi.get(tok, len(glove.stoi)-1) for tok in tokens]\n",
        "\t# Convert into a tensor of the shape accepted by the models\n",
        "\ttoken_tensor = torch.LongTensor(token_ints).view(-1,1)\n",
        "\t# Get the output from the model\n",
        "\tprediction = baseline_model(token_tensor)\n",
        "\tprediction = torch.sigmoid(prediction)\n",
        "\tprediction = prediction.item()\n",
        "\tif prediction > 0.5:\n",
        "\t\treturn prediction, \"Subjective\"\n",
        "\telse:\n",
        "\t\treturn prediction, \"Objective\"\n",
        "\n",
        "# cnn prediction function with output probability and label\n",
        "def cnn_predict(sentence):\n",
        "\ttokens = sentence.split()\n",
        "\t# Convert to integer representation per token\n",
        "\ttoken_ints = [glove.stoi.get(tok, len(glove.stoi)-1) for tok in tokens]\n",
        "\t# Convert into a tensor of the shape accepted by the models\n",
        "\ttoken_tensor = torch.LongTensor(token_ints).view(-1,1)\n",
        "\t# Get the output from the model\n",
        "\tprediction = cnn_model(token_tensor.flatten())\n",
        "\tprediction = torch.sigmoid(prediction)\n",
        "\tprediction = prediction.item()\n",
        "\tif prediction > 0.5:\n",
        "\t\treturn prediction, \"Subjective\"\n",
        "\telse:\n",
        "\t\treturn prediction, \"Objective\"\n",
        "\n",
        "# baseline interface\n",
        "baseline_interface = gr.Interface(fn=baseline_predict, inputs=\"text\", outputs= [gr.outputs.Textbox(label=\"Baseline Probability\"), gr.outputs.Textbox(label=\"Baseline Label\")], title=\"Baseline Model\", description=\"Enter a sentence to predict whether it is subjective or objective\", allow_flagging=False)\n",
        "\n",
        "\n",
        "# cnn interface\n",
        "cnn_interface = gr.Interface(fn=cnn_predict, inputs=\"text\", outputs=[gr.outputs.Textbox(label=\"CNN Probability\"), gr.outputs.Textbox(label=\"CNN Label\")], title=\"CNN Model\", description=\"Enter a sentence to predict whether it is subjective or objective.\", allow_flagging=False)\n",
        "\n",
        "\n",
        "gr.Parallel(baseline_interface, cnn_interface).launch(debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJRXtbNqh1Gx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a513deec7106273024eef7febe46d4fe4cdb642620a5b7c4bf01ad189c916154"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}