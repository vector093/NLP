# -*- coding: utf-8 -*-
"""A1P3_5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wao8AFKWr8EI1a4RxeWf2_BgjBSUHT1Q
"""

from collections import Counter
import numpy as np
import torch
import spacy
from sklearn.model_selection import train_test_split

import itertools
import random
import torch.optim as optim
import matplotlib.pyplot as plt

def tokenize_and_preprocess_text(textlist, v2i, window):
    data_list=[]
    # Predict context with word. Sample the context within a window size.
    X=[]
    Y=[]
    for sentence in textlist:
      for i,item in enumerate(sentence):
        #print(i, item)
        shift=window//2
        if(sentence[max(0,i-shift):i]):
          for word in sentence[max(0,i-shift):i]:
            X.append(v2i[item])
            Y.append(v2i[word])
        if(sentence[i+1:min(len(sentence),i+shift)+1]):
          for word in sentence[i+1:min(len(sentence),i+shift)+1]:
            X.append(v2i[item])
            Y.append(v2i[word])
        #print(sentence[i+1:min(len(sentence),i+shift)+1])
        
        #data_list.append(list(zip([item], textlist[max(0,i-window):min(len(textlist)-1,i+window)+1])))
      
    #X,Y=train_test_split(data_list,test_size=0.2,shuffle=True)
    # is the list of training/test samples
    
    # TO DO - create all the X,Y pairs
    
    return X,Y

class Word2vecModel(torch.nn.Module):
    def __init__(self, vocab_size, embedding_size,i2v,v2i):
        super().__init__()
        # initialize word vectors to random numbers 
        self.word_vectors=[]
        for _ in range(vocab_size):
          vectors=[random.random() for _ in range(embedding_size)]
          self.word_vectors.append(vectors)
        self.vocab_size=vocab_size
        self.embedding_size= embedding_size
        self.v2i=v2i
        self.i2v=i2v
        self.embedding=torch.nn.Embedding(self.vocab_size,self.embedding_size)
        self.out=torch.nn.Linear(self.embedding_size,self.vocab_size)
        
    def forward(self, x):
        """
        x: torch.tensor of shape (bsz), bsz is the batch size
        """
        #TO DO
        e=self.embedding(x)
        logits=self.out(e)
        out=torch.nn.functional.log_softmax(logits, dim=1)
        
        return out

def train_word2vec(textlist, window, embedding_size,v2i, i2v ):
    # Set up a model with Skip-gram (predict context with word)
    # textlist: a list of the strings
    vocab_size=len(v2i) #vocab size
    model=Word2vecModel(vocab_size,embedding_size,i2v,v2i) # set up model
    batch_size = 4  # fix batch size

    #preprocess text
    X,Y=tokenize_and_preprocess_text(textlist, v2i, window)

    #split data into 80:20 train:val
    X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.2,shuffle=True,random_state=11)

    X_train=torch.tensor(X_train)
    Y_train=torch.tensor(Y_train)
    X_test=torch.tensor(X_test)
    Y_test=torch.tensor(Y_test)

    #create train and validation sets
    trainset=torch.utils.data.TensorDataset(X_train,Y_train)
    testset=torch.utils.data.TensorDataset(X_test,Y_test)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                          shuffle=True)
    
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                         shuffle=False)
    
    #specify loss function and optimizer
    loss_func = torch.nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    train_losses=[]
    test_losses=[]
    x_axis=[]


    for epoch in range(50):  # loop over the dataset multiple times

      train_loss = 0.0
      model.train()
      x_axis.append(epoch)

      for inputs, labels in trainloader:
          # get the inputs; data is a list of [inputs, labels]
          #inputs, labels = data

          # zero the parameter gradients
          optimizer.zero_grad()

          # forward + backward + optimize
          outputs = model(inputs)
          #print(outputs)
          #print(labels)
          loss = loss_func(outputs, labels)
          loss.backward()
          optimizer.step()

          #train loss
          train_loss+= loss.item()
      train_loss=train_loss/len(trainloader)
      train_losses.append(train_loss)

      #validation
      test_loss=0
      model.eval()
      for inputs, labels in testloader:

        pred=model(inputs)
        loss = loss_func(pred, labels)
        test_loss+= loss.item()
      test_loss=test_loss/len(testloader)
      test_losses.append(test_loss)

    print('Finished Training')
    plt.plot(train_losses, label='train loss')
    plt.plot(test_losses, label='test loss')
    plt.legend()
    plt.show()


    return model