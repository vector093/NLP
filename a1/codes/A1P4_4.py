# -*- coding: utf-8 -*-
"""A1P4_4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wao8AFKWr8EI1a4RxeWf2_BgjBSUHT1Q
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from collections import Counter
import numpy as np
from nltk.tokenize import sent_tokenize
import time
import torch
import spacy
from pathlib import Path
from sklearn.model_selection import train_test_split
import torch.nn.functional as F

import nltk
nltk.download('punkt')

import random
import torch.optim as optim
import matplotlib.pyplot as plt

def tokenize_and_preprocess_text(textlist, w2i, window):
    """
    Skip-gram negative sampling: Predict if the target word is in the context.
    Uses binary prediction so we need both positive and negative samples
    """
    X, T, Y = [], [], []
    


    for i,item in enumerate(textlist):
      #print(i, item)
      shift=window//2
      if(textlist[max(0,i-shift):i]):
        for word in textlist[max(0,i-shift):i]:
          X.append(w2i[item])
          T.append(w2i[word])
          Y.append(1)
          X.append(random.randint(0, len(w2i)-1))
          T.append(random.randint(0, len(w2i)-1))
          Y.append(-1)

      if(textlist[i+1:min(len(textlist),i+shift)+1]):
        for word in textlist[i+1:min(len(textlist),i+shift)+1]:
          X.append(w2i[item])
          T.append(w2i[word])
          Y.append(1)
          X.append(random.randint(0, len(w2i)-1))
          T.append(random.randint(0, len(w2i)-1))
          Y.append(-1)
        #print(sentence[i+1:min(len(sentence),i+shift)+1])
        
        #data_list.append(list(zip([item], textlist[max(0,i-window):min(len(textlist)-1,i+window)+1])))
      
    #X,Y=train_test_split(data_list,test_size=0.2,shuffle=True)
    # is the list of training/test samples
    
    # TO DO - create all the X,Y pairs
    
    return X, T, Y