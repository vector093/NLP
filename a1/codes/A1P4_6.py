# -*- coding: utf-8 -*-
"""A1P4_6.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wao8AFKWr8EI1a4RxeWf2_BgjBSUHT1Q
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from collections import Counter
import numpy as np
from nltk.tokenize import sent_tokenize
import time
import torch
import spacy
from pathlib import Path
from sklearn.model_selection import train_test_split
import torch.nn.functional as F

import nltk
nltk.download('punkt')

import random
import torch.optim as optim
import matplotlib.pyplot as plt

class SkipGramNegativeSampling(torch.nn.Module):
    def __init__(self, vocab_size, embedding_size,w2i, i2w):
        super().__init__()
        self.vocab_size=vocab_size
        self.embedding_size= embedding_size
        self.w2i=w2i
        self.i2w=i2w
        self.embedding=torch.nn.Embedding(self.vocab_size,self.embedding_size)
        
        
    def forward(self, x, t):
        
        # x: torch.tensor of shape (batch_size), context word
        # t: torch.tensor of shape (batch_size), target ("output") word.
        
        e1=self.embedding(x)
        e2=self.embedding(t)
        prediction= torch.sum(e1* e2, dim=-1)
        #prediction=torch.nn.functional.sigmoid(out)
        # TO DO

        return prediction