# -*- coding: utf-8 -*-
"""A1P4_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wao8AFKWr8EI1a4RxeWf2_BgjBSUHT1Q
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
from collections import Counter
import numpy as np
from nltk.tokenize import sent_tokenize
import time
import torch
import spacy
from pathlib import Path
from sklearn.model_selection import train_test_split
import torch.nn.functional as F

import nltk
nltk.download('punkt')

import random
import torch.optim as optim
import matplotlib.pyplot as plt

def tokenize_and_preprocess_text(textlist, w2i, window):
    """
    Skip-gram negative sampling: Predict if the target word is in the context.
    Uses binary prediction so we need both positive and negative samples
    """
    X, T, Y = [], [], []
    


    for i,item in enumerate(textlist):
      #print(i, item)
      shift=window//2
      if(textlist[max(0,i-shift):i]):
        for word in textlist[max(0,i-shift):i]:
          X.append(w2i[item])
          T.append(w2i[word])
          Y.append(1)
          X.append(random.randint(0, len(w2i)-1))
          T.append(random.randint(0, len(w2i)-1))
          Y.append(-1)

      if(textlist[i+1:min(len(textlist),i+shift)+1]):
        for word in textlist[i+1:min(len(textlist),i+shift)+1]:
          X.append(w2i[item])
          T.append(w2i[word])
          Y.append(1)
          X.append(random.randint(0, len(w2i)-1))
          T.append(random.randint(0, len(w2i)-1))
          Y.append(-1)
        #print(sentence[i+1:min(len(sentence),i+shift)+1])
        
        #data_list.append(list(zip([item], textlist[max(0,i-window):min(len(textlist)-1,i+window)+1])))
      
    #X,Y=train_test_split(data_list,test_size=0.2,shuffle=True)
    # is the list of training/test samples
    
    # TO DO - create all the X,Y pairs
    
    return X, T, Y

class SkipGramNegativeSampling(torch.nn.Module):
    def __init__(self, vocab_size, embedding_size,w2i, i2w):
        super().__init__()
        self.vocab_size=vocab_size
        self.embedding_size= embedding_size
        self.w2i=w2i
        self.i2w=i2w
        self.embedding=torch.nn.Embedding(self.vocab_size,self.embedding_size)
        
        
    def forward(self, x, t):
        
        # x: torch.tensor of shape (batch_size), context word
        # t: torch.tensor of shape (batch_size), target ("output") word.
        
        e1=self.embedding(x)
        e2=self.embedding(t)
        prediction= torch.sum(e1* e2, dim=-1)
        #prediction=torch.nn.functional.sigmoid(out)
        # TO DO

        return prediction

def train_sgns(textlist, window, embedding_size,w2i, i2w):
    # Set up a model with Skip-gram with negative sampling (predict context with word)
    # textlist: a list of strings
    
    # instantiate the network & set up the optimizer
    vocab_size=len(w2i) #vocab size
    model=SkipGramNegativeSampling(vocab_size,embedding_size,w2i, i2w) #set up model
    batch_size = 4 #set batch size

    #preprocess text
    X,T,Y=tokenize_and_preprocess_text(textlist, w2i, window)

    #combine Xand T as inputs into model
    data=[]
    for i,x in enumerate(X):
      data.append((x,T[i]))

    #split data into train and validation
    X_train,X_test,Y_train,Y_test= train_test_split(data,Y,test_size=0.2,shuffle=True)

    X_train=torch.tensor(X_train)
    Y_train=torch.tensor(Y_train)
    X_test=torch.tensor(X_test)
    Y_test=torch.tensor(Y_test)

    #create train and validation sets
    trainset=torch.utils.data.TensorDataset(X_train,Y_train)
    testset=torch.utils.data.TensorDataset(X_test,Y_test)

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True)
    
    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False)
    
    #initilize loss function and optimizer
    loss_func = torch.nn.BCELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.0005)

    train_losses=[]
    test_losses=[]

    for epoch in range(30):  # loop over the dataset multiple times
      print('epoch=', epoch)
      train_loss = 0.0
      model.train()

      for inputs, labels in torch.utils.data.DataLoader(trainset, batch_size=batch_size,shuffle=True):

          # get the inputs; data is a list of [inputs, labels]
          #inputs, labels = data
          x1=torch.tensor([torch.IntTensor.item(i[0]) for i in inputs])
          x2=torch.tensor([torch.IntTensor.item(i[1]) for i in inputs])

          # zero the parameter gradients
          optimizer.zero_grad()

          # forward + backward + optimize
          outputs = model(x1,x2)
          #print(outputs)
          #labels=torch.nn.functional.sigmoid(labels)
          labels[labels==-1] = 0
          #print(labels)
          loss = loss_func(outputs, labels.to(torch.float32))
          loss.backward()
          optimizer.step()

          #train loss
          train_loss+= loss.item()
      train_loss=train_loss/len(trainloader)
      print('train loss=', train_loss)
      train_losses.append(train_loss)

      #validation
      test_loss=0
      model.eval()
      for inputs, labels in torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False):

        x1=torch.tensor([torch.IntTensor.item(i[0]) for i in inputs])
        x2=torch.tensor([torch.IntTensor.item(i[1]) for i in inputs])

        pred=model(x1,x2)
        labels[labels==-1] = 0
        loss = loss_func(pred, labels.to(torch.float32))
        test_loss+= loss.item()
      test_loss=test_loss/len(testloader)
      print('validation loss=', test_loss)
      test_losses.append(test_loss)

    print('Finished Training')
    plt.plot(train_losses, label='train loss')
    plt.plot(test_losses, label='validation loss')
    plt.legend()
    plt.show()


    return model