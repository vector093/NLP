{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use generate function on gpt2 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# output is 30 tokens long and print probability of each token\n",
    "output = model.generate(input_ids, max_length=30, do_sample=False, top_k=1, top_p=0.01, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedySearchDecoderOnlyOutput(sequences=tensor([[1026,  318, 1593,  329,  477, 2678,  284, 1949, 7069,  284, 4646, 6588,\n",
      "         8971,  780,  340,  318,  257, 1994,  636,  286,  262, 3298, 3773,   13,\n",
      "          198,  198,    1, 1135,  761,  284]]), scores=(tensor([[-126.3008, -125.6421, -133.5102,  ..., -132.7888, -133.6604,\n",
      "         -127.7045]]), tensor([[-132.1247, -130.5210, -138.0126,  ..., -140.9460, -139.2059,\n",
      "         -132.6104]]), tensor([[-135.8139, -135.3814, -143.6343,  ..., -143.6512, -141.7850,\n",
      "         -138.9233]]), tensor([[-147.2896, -145.7075, -151.7347,  ..., -157.4900, -153.2661,\n",
      "         -148.2787]]), tensor([[-113.9601, -111.4267, -118.7147,  ..., -121.8256, -122.8286,\n",
      "         -116.0324]]), tensor([[-61.5197, -57.9800, -64.9529,  ..., -70.8180, -72.1236, -62.2383]]), tensor([[-108.9445, -106.4659, -111.8005,  ..., -116.4430, -115.6365,\n",
      "         -108.9539]]), tensor([[-140.2385, -137.1673, -141.4753,  ..., -145.5077, -148.0972,\n",
      "         -139.7022]]), tensor([[-140.6001, -134.5880, -143.7945,  ..., -147.8121, -148.8864,\n",
      "         -140.5132]]), tensor([[-71.6851, -67.7631, -77.8021,  ..., -85.6815, -86.1894, -72.1126]]), tensor([[-161.8446, -159.3154, -161.3855,  ..., -177.5978, -178.4566,\n",
      "         -152.9373]]), tensor([[-267.1910, -261.1628, -267.5240,  ..., -287.1937, -294.8809,\n",
      "         -262.8905]]), tensor([[-127.5040, -115.4984, -121.2265,  ..., -144.9753, -145.8369,\n",
      "         -128.6196]]), tensor([[-153.9667, -148.1164, -150.3910,  ..., -171.0219, -172.3200,\n",
      "         -150.9536]]), tensor([[-150.3007, -147.4449, -152.8656,  ..., -157.8536, -152.8102,\n",
      "         -149.6004]]), tensor([[-12.4335, -10.6671, -15.8367,  ..., -20.5065, -20.7824, -14.3128]])), attentions=None, hidden_states=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(output.scores[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax of output scores\n",
    "softmax = torch.nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(198)\n",
      "tensor(0.2911)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(softmax(output.scores[23][0])))\n",
    "print(max(softmax(output.scores[23][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# store the top 10 tokens and their probabilities from output scores\n",
    "top10 = []\n",
    "for i in range(len(output.scores[:])):\n",
    "\t# softmax of output scores\n",
    "\tsoftmax = torch.nn.Softmax(dim=0)\n",
    "\t# get the top 10 tokens and their probabilities\n",
    "\tsoft = softmax(output.scores[i][0])\n",
    "\ttop10.append(torch.topk(soft, 3))\n",
    "# print shape of top10\n",
    "print(len(top10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([257, 262, 407])\n"
     ]
    }
   ],
   "source": [
    "print(top10[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "├──  it\n",
      "│   ├──  can\n",
      "│   ├──  is\n",
      "│   └──  will\n",
      "├──  the\n",
      "│   ├──  climate\n",
      "│   ├──  global\n",
      "│   └──  world\n",
      "└──  they\n",
      "    ├──  are\n",
      "    ├──  have\n",
      "    └──  will\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import treelib\n",
    "# get top3 tokens and their probabilities at each position for depth 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "text = prompt\n",
    "# softmax of output scores\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "# tree creation\n",
    "tree = treelib.Tree()\n",
    "tree.create_node(\"root\", \"root\")\n",
    "for i in range(1):\n",
    "\n",
    "\tinput_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\t# output is 30 tokens long and print probability of each token\n",
    "\toutput = model.generate(input_ids, max_length=20, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "\t# get the top 10 tokens and their probabilities\n",
    "\tsoft = softmax(output.scores[0][0])\n",
    "\twords = torch.topk(soft, 3)\n",
    "\t# print(words[1][0])\n",
    "\tword = tokenizer.batch_decode(words[1], skip_special_tokens=True)\n",
    "\t# word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "\t# word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "\tfor j in range(3):\n",
    "\t\ttext = prompt + \" \" + word[j]\n",
    "\t\t# print(text)\n",
    "\t\ttree.create_node(word[j], \"j\"+str(j), parent=\"root\", data=words[0][j])\n",
    "\t\tinput_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\t\t# output is 30 tokens long and print probability of each token\n",
    "\t\toutput = model.generate(input_ids, max_length=20, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "\t\t# get the top 10 tokens and their probabilities\n",
    "\t\tsoft = softmax(output.scores[0][0])\n",
    "\t\twords_new = torch.topk(soft, 3)\n",
    "\t\t# print(words[1][0])\n",
    "\t\tword_new = tokenizer.batch_decode(words_new[1], skip_special_tokens=True)\n",
    "\t\t# word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "\t\t# word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "\t\tfor k in range(3):\n",
    "\t\t\t# print(t)\n",
    "\t\t\ttree.create_node(word_new[k], \"k\"+str(k)+\"j\"+str(j), parent=\"j\"+str(j), data=words_new[0][k])\n",
    "tree.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eae829583d2fb097fbd757bc95a0f4009c916c9b7dbc1c8b2d2d262108f06b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
