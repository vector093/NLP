{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Why is model conversion important?',\n",
    "    'context': 'The option to convert models between FARM and transformers gives freedom to the user and let people easily switch between frameworks.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.21171414852142334, 'start': 59, 'end': 84, 'answer': 'gives freedom to the user'}\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.490986704826355, 'start': 388, 'end': 392, 'answer': 'bole'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# a) Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Which part of a tree can have vertical or horizontal variation in its specific gravity?',\n",
    "    'context': 'Age, diameter, height, radial growth, geographical location, site and growing conditions, silvicultural treatment, and seed source, all to some degree influence wood density. Variation is to be expected. Within an individual tree, the variation in wood density is often as great as or even greater than that between different trees (Timell 1986). Variation of specific gravity within the bole of a tree can occur in either the horizontal or vertical direction.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "\n",
    "# b) Load model & tokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Today I believe we can finally\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# sample up to 30 tokens\n",
    "torch.manual_seed(0)\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "['Today I believe we can finally get rid of discrimination,\" said Rep. Mark Pocan (D-Wis.).\\n\\n\"Just look at the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is important for all countries to try harder to reduce carbon emissions because it is a very big problem in the world today,\" he said.\\n\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model generate max 30 tokens and temperature 0.7 and top_p 0.9\n",
    "torch.manual_seed(0)\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30,top_p=0.9, temperature=0.7)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "# It is important for all countries to try harder to reduce carbon emissions because it is a very big problem in the world today,\" he said.\\n\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is important for all countries to try harder to reduce carbon emissions because this is the only way to tackle climate change,\" he said.\\n\\n\"']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model generate max 30 tokens and temperature 0.8 and top_p 0.8\n",
    "torch.manual_seed(0)\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, do_sample=True, max_length=30,top_p=0.8, temperature=0.8)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "# ['It is important for all countries to try harder to reduce carbon emissions because this is the only way to tackle climate change,\" he said.\\n\\n\"']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the probabilities of the each word that is generated.\n",
    "torch.manual_seed(0)\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=30,top_p=0.1, temperature=1.0, return_dict_in_generate=True, output_scores=True)\n",
    "tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedySearchDecoderOnlyOutput(sequences=tensor([[1026,  318, 1593,  329,  477, 2678,  284, 1949, 7069,  284, 4646, 6588,\n",
      "         8971,  780,  340,  318,  257, 1994,  636,  286,  262, 3298, 3773,   13,\n",
      "          198,  198,    1, 1135,  761,  284]]), scores=(tensor([[-126.3008, -125.6421, -133.5102,  ..., -132.7888, -133.6604,\n",
      "         -127.7045]]), tensor([[-132.1247, -130.5210, -138.0126,  ..., -140.9460, -139.2059,\n",
      "         -132.6104]]), tensor([[-135.8139, -135.3814, -143.6343,  ..., -143.6512, -141.7850,\n",
      "         -138.9233]]), tensor([[-147.2896, -145.7075, -151.7347,  ..., -157.4900, -153.2661,\n",
      "         -148.2787]]), tensor([[-113.9601, -111.4267, -118.7147,  ..., -121.8256, -122.8286,\n",
      "         -116.0324]]), tensor([[-61.5197, -57.9800, -64.9529,  ..., -70.8180, -72.1236, -62.2383]]), tensor([[-108.9445, -106.4659, -111.8005,  ..., -116.4430, -115.6365,\n",
      "         -108.9539]]), tensor([[-140.2385, -137.1673, -141.4753,  ..., -145.5077, -148.0972,\n",
      "         -139.7022]]), tensor([[-140.6001, -134.5880, -143.7945,  ..., -147.8121, -148.8864,\n",
      "         -140.5132]]), tensor([[-71.6851, -67.7631, -77.8021,  ..., -85.6815, -86.1894, -72.1126]]), tensor([[-161.8446, -159.3154, -161.3855,  ..., -177.5978, -178.4566,\n",
      "         -152.9373]]), tensor([[-267.1910, -261.1628, -267.5240,  ..., -287.1937, -294.8809,\n",
      "         -262.8905]]), tensor([[-127.5040, -115.4984, -121.2265,  ..., -144.9753, -145.8369,\n",
      "         -128.6196]]), tensor([[-153.9667, -148.1164, -150.3910,  ..., -171.0219, -172.3200,\n",
      "         -150.9536]]), tensor([[-150.3007, -147.4449, -152.8656,  ..., -157.8536, -152.8102,\n",
      "         -149.6004]]), tensor([[-12.4335, -10.6671, -15.8367,  ..., -20.5065, -20.7824, -14.3128]])), attentions=None, hidden_states=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modify the code to output the probabilities of the each word that is generated. You’ll need to set these two generate parameters:return_dict_in_generate=True and, output_scores=True, and extract the probabilities that come in the returned dictionary one call at a time. \n",
    "\n",
    "# output the probabilities of the each word that is generated.\n",
    "torch.manual_seed(0)\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=25,top_p=1.0, temperature=0.6, return_dict_in_generate=True, output_scores=True)\n",
    "tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['sequences', 'scores'])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probabilities of the each word that is generated.\n",
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]),\n",
       " tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# output the probabilities of the each word that is generated. You’ll need to set these two generate parameters:return_dict_in_generate=True and, output_scores=True, and extract the probabilities that come in the returned dictionary one call at a time.\n",
    "outputs = model.generate(input_ids, max_length=25,top_p=0.1, temperature=1.0, return_dict_in_generate=True, output_scores=True, do_sample=True)\n",
    "tokenizer.batch_decode(outputs.sequences, skip_special_tokens=True)\n",
    "\n",
    "# probabilities of the each word that is generated.\n",
    "outputs.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SampleDecoderOnlyOutput(sequences=tensor([[1026,  318, 1593,  329,  477, 2678,  284, 1949, 7069,  284, 4646, 6588,\n",
      "         8971,  780,  340,  318,  257, 1994,  636,  286,  262, 3298, 3773,   13,\n",
      "          198]]), scores=(tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]]), tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf]])), attentions=None, hidden_states=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [46], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i, word \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tokenizer\u001b[39m.\u001b[39mbatch_decode(outputs\u001b[39m.\u001b[39msequences, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit()):\n\u001b[1;32m     10\u001b[0m \ttree\u001b[39m.\u001b[39mcreate_node(word, word, parent\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \t\u001b[39mfor\u001b[39;00m j, prob \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(probabilities[\u001b[39m0\u001b[39;49m][i]):\n\u001b[1;32m     12\u001b[0m \t\ttree\u001b[39m.\u001b[39mcreate_node(prob, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mj\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, parent\u001b[39m=\u001b[39mword)\n\u001b[1;32m     14\u001b[0m tree\u001b[39m.\u001b[39mshow()\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "# Generate the tree for the above sequence as input, providing the top 3 probabilities for each word position\n",
    "probabilities = outputs.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eae829583d2fb097fbd757bc95a0f4009c916c9b7dbc1c8b2d2d262108f06b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
