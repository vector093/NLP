{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wM4sQcBy2j75",
        "outputId": "75db8b0a-6fd6-4152-f53b-6c4ca31450db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 15.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
            "\u001b[K     |████████████████████████████████| 163 kB 60.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 39.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install treelib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NIEcM0yRGGh",
        "outputId": "b7bb068f-e924-4c5c-8a9e-75cc8eb87b5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting treelib\n",
            "  Downloading treelib-1.6.1.tar.gz (24 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from treelib) (0.16.0)\n",
            "Building wheels for collected packages: treelib\n",
            "  Building wheel for treelib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for treelib: filename=treelib-1.6.1-py3-none-any.whl size=18385 sha256=242efc566804fbaad636bec7689f3c1e71ae390deb41de1b30dffa31782b3f71\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/be/94/2c6d949ce599d1443426d83ba4dc93cd35c0f4638260930a53\n",
            "Successfully built treelib\n",
            "Installing collected packages: treelib\n",
            "Successfully installed treelib-1.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xl5gXqtk2aJj"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#exploring temperature and top_p parameters\n",
        "\n",
        "# use generate function on gpt2 model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "\n",
        "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "#max of 30 tokens\n",
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.01, temperature=0.9)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43fxv8lq3Wkt",
        "outputId": "24f530d6-17ac-4fb3-ff92-2342191f739c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.01, temperature=0.7)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuUv8VPe4D85",
        "outputId": "fb5f4127-42da-452b-d9fa-10a6b7979482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.01, temperature=0.2)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrddaBqxCBmF",
        "outputId": "619044b3-ad0d-499e-fe75-c3f399a578ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.2, temperature=0.7)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TsH3ikdG4ShT",
        "outputId": "a447451c-0ce2-483d-9858-d414a17d088a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.5, temperature=0.7)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocud1w154XuK",
        "outputId": "29700b41-29ae-40f9-ce0a-083fd79d1d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because they are the main source of greenhouse gas emissions,\" said Dr. Jens-']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.9, temperature=0.7)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHarWCd34gnq",
        "outputId": "bc82dcf2-2302-4a04-e391-318503362445"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because it is the most important thing to do,\" he said.\\n\\n\"This']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.5, temperature=1)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdAFnx3I4lBs",
        "outputId": "a598b9a0-8e61-4b7d-db19-9c6d5f304bc7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because the global economy is still growing at a rapid rate, and the climate is changing']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.5, temperature=2.0)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pp7XzyV4rBm",
        "outputId": "993b3b2d-239e-4799-92a2-848da2845c28"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because we all know that emissions are growing fast and we need to tackle them.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.7, temperature=1)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfyEQ60t9ee0",
        "outputId": "fa376012-a06f-405d-f9fd-0ca20fc69075"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because that is what is going to happen in the future,\" he said. \"It']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(input_ids, max_length=30, do_sample=True, top_p=0.2, temperature=4.0)\n",
        "tokenizer.batch_decode(output, skip_special_tokens=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDo7MiFSDu9T",
        "outputId": "89d1c532-55df-419d-cff9-b5733c9e3a57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because we can make progress,\" she said. \"But we need the world to stop']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dNFbOiB2aJn",
        "outputId": "c9fe0c46-1f96-4fea-c091-ce6e12a42434"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "#finding the probabilities of all words without using sampling\n",
        "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=30, do_sample=False, top_p=0.2, temperature=0.7, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
        "tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lOjKyEt2aJp",
        "outputId": "6422f600-5eaa-4d8d-f598-0c31cca2bcf8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GreedySearchDecoderOnlyOutput(sequences=tensor([[1026,  318, 1593,  329,  477, 2678,  284, 1949, 7069,  284, 4646, 6588,\n",
            "         8971,  780,  340,  318,  257, 1994,  636,  286,  262, 3298, 3773,   13,\n",
            "          198,  198,    1, 1135,  761,  284]]), scores=(tensor([[-126.3009, -125.6422, -133.5102,  ..., -132.7887, -133.6604,\n",
            "         -127.7045]]), tensor([[-132.1247, -130.5210, -138.0126,  ..., -140.9460, -139.2059,\n",
            "         -132.6104]]), tensor([[-135.8139, -135.3813, -143.6343,  ..., -143.6512, -141.7850,\n",
            "         -138.9233]]), tensor([[-147.2896, -145.7075, -151.7347,  ..., -157.4901, -153.2661,\n",
            "         -148.2787]]), tensor([[-113.9601, -111.4267, -118.7147,  ..., -121.8256, -122.8286,\n",
            "         -116.0324]]), tensor([[-61.5194, -57.9797, -64.9526,  ..., -70.8178, -72.1233, -62.2381]]), tensor([[-108.9445, -106.4659, -111.8005,  ..., -116.4431, -115.6365,\n",
            "         -108.9540]]), tensor([[-140.2385, -137.1672, -141.4753,  ..., -145.5076, -148.0972,\n",
            "         -139.7022]]), tensor([[-140.6001, -134.5880, -143.7945,  ..., -147.8121, -148.8864,\n",
            "         -140.5133]]), tensor([[-71.6851, -67.7630, -77.8021,  ..., -85.6815, -86.1894, -72.1126]]), tensor([[-161.8445, -159.3153, -161.3854,  ..., -177.5977, -178.4566,\n",
            "         -152.9373]]), tensor([[-267.1910, -261.1627, -267.5240,  ..., -287.1936, -294.8808,\n",
            "         -262.8905]]), tensor([[-127.5039, -115.4984, -121.2264,  ..., -144.9753, -145.8369,\n",
            "         -128.6196]]), tensor([[-153.9667, -148.1164, -150.3910,  ..., -171.0218, -172.3200,\n",
            "         -150.9536]]), tensor([[-150.3007, -147.4449, -152.8656,  ..., -157.8536, -152.8102,\n",
            "         -149.6004]]), tensor([[-12.4334, -10.6669, -15.8365,  ..., -20.5063, -20.7822, -14.3126]])), attentions=None, hidden_states=None)\n"
          ]
        }
      ],
      "source": [
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fiOR5mB2aJp",
        "outputId": "d4f8c72b-d887-4b78-8e92-75d1a4ee5bc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16\n"
          ]
        }
      ],
      "source": [
        "print(len(output.scores[:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWJCs-GO2aJq"
      },
      "outputs": [],
      "source": [
        "# softmax of output scores\n",
        "softmax = torch.nn.Softmax(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sczHwu352aJq",
        "outputId": "a18d6ddf-46d8-481e-c11c-8bda1dae4443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word 1 :  it \tprobability: 0.16149355471134186\n",
            "word 2 :  is \tprobability: 0.33389467000961304\n",
            "word 3 :  a \tprobability: 0.15053123235702515\n",
            "word 4 :  key \tprobability: 0.06983855366706848\n",
            "word 5 :  part \tprobability: 0.15043514966964722\n",
            "word 6 :  of \tprobability: 0.9890421032905579\n",
            "word 7 :  the \tprobability: 0.264200896024704\n",
            "word 8 :  global \tprobability: 0.11687494069337845\n",
            "word 9 :  economy \tprobability: 0.19671541452407837\n",
            "word 10 : . \tprobability: 0.3323745131492615\n",
            "word 11 : \n",
            " \tprobability: 0.21852098405361176\n",
            "word 12 : \n",
            " \tprobability: 0.999492883682251\n",
            "word 13 : \" \tprobability: 0.28310999274253845\n",
            "word 14 : We \tprobability: 0.14027848839759827\n",
            "word 15 :  need \tprobability: 0.23627544939517975\n",
            "word 16 :  to \tprobability: 0.7871111631393433\n"
          ]
        }
      ],
      "source": [
        "for i in range(16):\n",
        "\tprint('word',i+1,':', tokenizer.decode(torch.argmax(softmax(output.scores[i][0])), skip_special_tokens=True),'\t' 'probability:',max(softmax(output.scores[i][0])).item())\n",
        "\t#print(max(softmax(output.scores[i][0])))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "# softmax of output scores\n",
        "softmax = torch.nn.Softmax(dim=0)\n",
        "\n",
        "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\""
      ],
      "metadata": {
        "id": "NRaBO5BWQ_SB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "pIXfsuZK2aJs",
        "outputId": "be760cb9-672f-4f81-af83-a6e8ac55e3e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            "├──  it\n",
            "│   ├──  can\n",
            "│   │   ├──  be\n",
            "│   │   ├──  help\n",
            "│   │   └──  reduce\n",
            "│   ├──  is\n",
            "│   │   ├──  a\n",
            "│   │   ├──  important\n",
            "│   │   └──  the\n",
            "│   └──  will\n",
            "│       ├──  be\n",
            "│       ├──  help\n",
            "│       └──  reduce\n",
            "├──  the\n",
            "│   ├──  climate\n",
            "│   │   ├──  change\n",
            "│   │   ├──  crisis\n",
            "│   │   └──  is\n",
            "│   ├──  global\n",
            "│   │   ├──  \n",
            "│   │   ├──  climate\n",
            "│   │   └──  warming\n",
            "│   └──  world\n",
            "│       ├──  is\n",
            "│       ├──  needs\n",
            "│       └── 's\n",
            "└──  they\n",
            "    ├──  are\n",
            "    │   ├──  a\n",
            "    │   ├──  not\n",
            "    │   └──  the\n",
            "    ├──  have\n",
            "    │   ├──  a\n",
            "    │   ├──  the\n",
            "    │   └──  to\n",
            "    └──  will\n",
            "        ├──  be\n",
            "        ├──  have\n",
            "        └──  not\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import treelib\n",
        "# get top3 tokens and their probabilities at each position for depth 3\n",
        "\n",
        "# tree creation\n",
        "tree = treelib.Tree()\n",
        "tree.create_node(\"root\", \"root\")\n",
        "\n",
        "# create output using prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "output = model.generate(input_ids, max_length=30, do_sample=False, top_p=0.1, temperature=1.5, return_dict_in_generate=True, output_scores=True) # set do sample to true for sampling\n",
        "\n",
        "# get the top 3 tokens based on their probabilities\n",
        "prob = softmax(output.scores[0][0])\n",
        "top_words = torch.topk(prob, 3)\n",
        "\n",
        "word = tokenizer.batch_decode(top_words[1], skip_special_tokens=True)\n",
        "\n",
        "for i in range(3):\n",
        "\t#update prompt with choosen word\n",
        "\tprompt2 = prompt + \" \" + word[i]\n",
        "\n",
        "\t#create node\n",
        "\ttree.create_node(word[i], \"i\"+str(i), parent=\"root\", data=top_words[0][i])\n",
        "\tinput_ids = tokenizer.encode(prompt2, return_tensors=\"pt\")\n",
        " \n",
        "\t# create output using new prompt\n",
        "\toutput = model.generate(input_ids, max_length=30, do_sample=False, top_p=0.1, temperature=1.5, return_dict_in_generate=True, output_scores=True) # set do sample to true for sampling\n",
        " \n",
        "\t# get the top 3 tokens based on their probabilities\n",
        "\tprob = softmax(output.scores[0][0])\n",
        "\ttop_words2 = torch.topk(prob, 3)\n",
        "\n",
        "\tword2 = tokenizer.batch_decode(top_words2[1], skip_special_tokens=True)\n",
        "\n",
        "\tfor j in range(3):\n",
        "\t\t#create node\n",
        "\t\ttree.create_node(word2[j], \"j\"+str(j)+\"i\"+str(i), parent=\"i\"+str(i), data=top_words2[0][j])\n",
        "\t\n",
        "\t\t#update prompt with choosen word\n",
        "\t\tprompt3 = prompt + \" \" + word[i] + \" \" + word2[j]\n",
        "\t\tinput_ids = tokenizer.encode(prompt3, return_tensors=\"pt\")\n",
        "\t\n",
        "\t\t# create output using new prompt\n",
        "\t\toutput = model.generate(input_ids, max_length=30, do_sample=False, top_p=0.1, temperature=1.5, return_dict_in_generate=True, output_scores=True) # set do sample to true for sampling\n",
        "\t\n",
        "\t\t# get the top 3 tokens based on their probabilities\n",
        "\t\tprob = softmax(output.scores[0][0])\n",
        "\t\ttop_words3 = torch.topk(prob, 3)\n",
        "\t\t\n",
        "\t\tword3 = tokenizer.batch_decode(top_words3[1], skip_special_tokens=True)\n",
        "\t\t\n",
        "\t\tfor k in range(3):\n",
        "\t\t\t#create node\n",
        "\t\t\ttree.create_node(word3[k], \"k\"+str(k)+\"j\"+str(j)+\"i\"+str(i), parent=\"j\"+str(j)+\"i\"+str(i), data=top_words3[0][k])\n",
        "\t\t\n",
        "tree.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.7 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "eae829583d2fb097fbd757bc95a0f4009c916c9b7dbc1c8b2d2d262108f06b92"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}