{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['It is important for all countries to try harder to reduce carbon emissions because it is a key part of the global economy.\\n\\n\"We need to']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use generate function on gpt2 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# output is 30 tokens long and print probability of each token\n",
    "output = model.generate(input_ids, max_length=30, do_sample=False, top_k=1, top_p=0.01, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GreedySearchDecoderOnlyOutput(sequences=tensor([[1026,  318, 1593,  329,  477, 2678,  284, 1949, 7069,  284, 4646, 6588,\n",
      "         8971,  780,  340,  318,  257, 1994,  636,  286,  262, 3298, 3773,   13,\n",
      "          198,  198,    1, 1135,  761,  284]]), scores=(tensor([[-126.3008, -125.6421, -133.5102,  ..., -132.7888, -133.6604,\n",
      "         -127.7045]]), tensor([[-132.1247, -130.5210, -138.0126,  ..., -140.9460, -139.2059,\n",
      "         -132.6104]]), tensor([[-135.8139, -135.3814, -143.6343,  ..., -143.6512, -141.7850,\n",
      "         -138.9233]]), tensor([[-147.2896, -145.7075, -151.7347,  ..., -157.4900, -153.2661,\n",
      "         -148.2787]]), tensor([[-113.9601, -111.4267, -118.7147,  ..., -121.8256, -122.8286,\n",
      "         -116.0324]]), tensor([[-61.5197, -57.9800, -64.9529,  ..., -70.8180, -72.1236, -62.2383]]), tensor([[-108.9445, -106.4659, -111.8005,  ..., -116.4430, -115.6365,\n",
      "         -108.9539]]), tensor([[-140.2385, -137.1673, -141.4753,  ..., -145.5077, -148.0972,\n",
      "         -139.7022]]), tensor([[-140.6001, -134.5880, -143.7945,  ..., -147.8121, -148.8864,\n",
      "         -140.5132]]), tensor([[-71.6851, -67.7631, -77.8021,  ..., -85.6815, -86.1894, -72.1126]]), tensor([[-161.8446, -159.3154, -161.3855,  ..., -177.5978, -178.4566,\n",
      "         -152.9373]]), tensor([[-267.1910, -261.1628, -267.5240,  ..., -287.1937, -294.8809,\n",
      "         -262.8905]]), tensor([[-127.5040, -115.4984, -121.2265,  ..., -144.9753, -145.8369,\n",
      "         -128.6196]]), tensor([[-153.9667, -148.1164, -150.3910,  ..., -171.0219, -172.3200,\n",
      "         -150.9536]]), tensor([[-150.3007, -147.4449, -152.8656,  ..., -157.8536, -152.8102,\n",
      "         -149.6004]]), tensor([[-12.4335, -10.6671, -15.8367,  ..., -20.5065, -20.7824, -14.3128]])), attentions=None, hidden_states=None)\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(output.scores[:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax of output scores\n",
    "softmax = torch.nn.Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(340)\n",
      "tensor(0.1615)\n",
      "tensor(318)\n",
      "tensor(0.3339)\n",
      "tensor(257)\n",
      "tensor(0.1505)\n",
      "tensor(1994)\n",
      "tensor(0.0698)\n",
      "tensor(636)\n",
      "tensor(0.1504)\n",
      "tensor(286)\n",
      "tensor(0.9890)\n",
      "tensor(262)\n",
      "tensor(0.2642)\n",
      "tensor(3298)\n",
      "tensor(0.1169)\n",
      "tensor(3773)\n",
      "tensor(0.1967)\n",
      "tensor(13)\n",
      "tensor(0.3324)\n",
      "tensor(198)\n",
      "tensor(0.2185)\n",
      "tensor(198)\n",
      "tensor(0.9995)\n",
      "tensor(1)\n",
      "tensor(0.2831)\n",
      "tensor(1135)\n",
      "tensor(0.1403)\n",
      "tensor(761)\n",
      "tensor(0.2363)\n",
      "tensor(284)\n",
      "tensor(0.7871)\n"
     ]
    }
   ],
   "source": [
    "for i in range(16):\n",
    "\tprint(torch.argmax(softmax(output.scores[i][0])))\n",
    "\tprint(max(softmax(output.scores[i][0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "# store the top 10 tokens and their probabilities from output scores\n",
    "top10 = []\n",
    "for i in range(len(output.scores[:])):\n",
    "\t# softmax of output scores\n",
    "\tsoftmax = torch.nn.Softmax(dim=0)\n",
    "\t# get the top 10 tokens and their probabilities\n",
    "\tsoft = softmax(output.scores[i][0])\n",
    "\ttop10.append(torch.topk(soft, 3))\n",
    "# print shape of top10\n",
    "print(len(top10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([257, 262, 407])\n"
     ]
    }
   ],
   "source": [
    "print(top10[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "├──  it\n",
      "│   ├──  can\n",
      "│   │   ├──  be\n",
      "│   │   │   ├──  a\n",
      "│   │   │   ├──  difficult\n",
      "│   │   │   └──  very\n",
      "│   │   ├──  help\n",
      "│   │   │   ├──  reduce\n",
      "│   │   │   ├──  to\n",
      "│   │   │   └──  us\n",
      "│   │   └──  reduce\n",
      "│   │       ├──  emissions\n",
      "│   │       ├──  greenhouse\n",
      "│   │       └──  the\n",
      "│   ├──  is\n",
      "│   │   ├──  a\n",
      "│   │   │   ├──  key\n",
      "│   │   │   ├──  major\n",
      "│   │   │   └──  very\n",
      "│   │   ├──  important\n",
      "│   │   │   ├──  for\n",
      "│   │   │   ├──  that\n",
      "│   │   │   └──  to\n",
      "│   │   └──  the\n",
      "│   │       ├──  cheapest\n",
      "│   │       ├──  most\n",
      "│   │       └──  only\n",
      "│   └──  will\n",
      "│       ├──  be\n",
      "│       │   ├──  a\n",
      "│       │   ├──  difficult\n",
      "│       │   └──  very\n",
      "│       ├──  help\n",
      "│       │   ├──  reduce\n",
      "│       │   ├──  to\n",
      "│       │   └──  us\n",
      "│       └──  reduce\n",
      "│           ├──  global\n",
      "│           ├──  greenhouse\n",
      "│           └──  the\n",
      "├──  the\n",
      "│   ├──  climate\n",
      "│   │   ├──  change\n",
      "│   │   │   ├──  \n",
      "│   │   │   ├──  is\n",
      "│   │   │   └──  will\n",
      "│   │   ├──  crisis\n",
      "│   │   │   ├──  \n",
      "│   │   │   ├──  has\n",
      "│   │   │   └──  is\n",
      "│   │   └──  is\n",
      "│   │       ├──  \n",
      "│   │       ├──  changing\n",
      "│   │       └──  not\n",
      "│   ├──  global\n",
      "│   │   ├──  \n",
      "│   │   │   ├──  \n",
      "│   │   │   ├── vernment\n",
      "│   │   │   └──  \n",
      "│   │   ├──  climate\n",
      "│   │   │   ├──  \n",
      "│   │   │   ├──  change\n",
      "│   │   │   └──  is\n",
      "│   │   └──  warming\n",
      "│   │       ├──  \n",
      "│   │       ├──  is\n",
      "│   │       └──  will\n",
      "│   └──  world\n",
      "│       ├──  is\n",
      "│       │   ├──  \n",
      "│       │   ├──  changing\n",
      "│       │   └──  warming\n",
      "│       ├──  needs\n",
      "│       │   ├──  \n",
      "│       │   ├──  more\n",
      "│       │   └──  to\n",
      "│       └── 's\n",
      "│           ├──  climate\n",
      "│           ├──  emissions\n",
      "│           └──  most\n",
      "└──  they\n",
      "    ├──  are\n",
      "    │   ├──  a\n",
      "    │   │   ├──  global\n",
      "    │   │   ├──  key\n",
      "    │   │   └──  major\n",
      "    │   ├──  not\n",
      "    │   │   ├──  \n",
      "    │   │   ├──  going\n",
      "    │   │   └──  the\n",
      "    │   └──  the\n",
      "    │       ├──  main\n",
      "    │       ├──  most\n",
      "    │       └──  only\n",
      "    ├──  have\n",
      "    │   ├──  a\n",
      "    │   │   ├──  lot\n",
      "    │   │   ├──  strong\n",
      "    │   │   └──  very\n",
      "    │   ├──  the\n",
      "    │   │   ├──  capacity\n",
      "    │   │   ├──  most\n",
      "    │   │   └──  resources\n",
      "    │   └──  to\n",
      "    │       ├──  be\n",
      "    │       ├──  do\n",
      "    │       └──  reduce\n",
      "    └──  will\n",
      "        ├──  be\n",
      "        │   ├──  able\n",
      "        │   ├──  more\n",
      "        │   └──  the\n",
      "        ├──  have\n",
      "        │   ├──  a\n",
      "        │   ├──  the\n",
      "        │   └──  to\n",
      "        └──  not\n",
      "            ├──  \n",
      "            ├──  be\n",
      "            └──  only\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import treelib\n",
    "# get top3 tokens and their probabilities at each position for depth 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "prompt = \"It is important for all countries to try harder to reduce carbon emissions because\"\n",
    "# softmax of output scores\n",
    "softmax = torch.nn.Softmax(dim=0)\n",
    "# tree creation\n",
    "tree = treelib.Tree()\n",
    "tree.create_node(\"root\", \"root\")\n",
    "for i in range(1):\n",
    "\tinput_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "\t# output is 30 tokens long and print probability of each token\n",
    "\toutput = model.generate(input_ids, max_length=20, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "\t# get the top 10 tokens and their probabilities\n",
    "\tsoft = softmax(output.scores[0][0])\n",
    "\twords = torch.topk(soft, 3)\n",
    "\t# print(words[1][0])\n",
    "\tword = tokenizer.batch_decode(words[1], skip_special_tokens=True)\n",
    "\t# word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "\t# word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "\tfor j in range(3):\n",
    "\t\ttext = prompt + \" \" + word[j]\n",
    "\t\t# print(text)\n",
    "\t\ttree.create_node(word[j], \"j\"+str(j), parent=\"root\", data=words[0][j])\n",
    "\t\tinput_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\t\t# output is 30 tokens long and print probability of each token\n",
    "\t\toutput = model.generate(input_ids, max_length=20, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "\t\t# get the top 10 tokens and their probabilities\n",
    "\t\tsoft = softmax(output.scores[0][0])\n",
    "\t\twords_new = torch.topk(soft, 3)\n",
    "\t\t# print(words[1][0])\n",
    "\t\tword_new = tokenizer.batch_decode(words_new[1], skip_special_tokens=True)\n",
    "\t\t# word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "\t\t# word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "\t\tfor k in range(3):\n",
    "\t\t\t# print(t)\n",
    "\t\t\ttree.create_node(word_new[k], \"k\"+str(k)+\"j\"+str(j), parent=\"j\"+str(j), data=words_new[0][k])\n",
    "\t\t\ttext = prompt + \" \" + word[j] + \" \" + word_new[k]\n",
    "\t\t\tinput_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\t\t\t# output is 30 tokens long and print probability of each token\n",
    "\t\t\toutput = model.generate(input_ids, max_length=30, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "\t\t\t# get the top 10 tokens and their probabilities\n",
    "\t\t\tsoft = softmax(output.scores[0][0])\n",
    "\t\t\twords_new2 = torch.topk(soft, 3)\n",
    "\t\t\t# print(words[1][0])\n",
    "\t\t\tword_new2 = tokenizer.batch_decode(words_new2[1], skip_special_tokens=True)\n",
    "\t\t\t# word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "\t\t\t# word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "\t\t\tfor l in range(3):\n",
    "\t\t\t\t# print(t)\n",
    "\t\t\t\ttree.create_node(word_new2[l], \"l\"+str(l)+\"k\"+str(k)+\"j\"+str(j), parent=\"k\"+str(k)+\"j\"+str(j), data=words_new2[0][l])\n",
    "\t\t\t\ttext = prompt + \" \" + word[j] + \" \" + word_new[k] + \" \" + word_new2[l]\n",
    "\t\t\t\tinput_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\t\t\t\t# output is 30 tokens long and print probability of each token\n",
    "\t\t\t\toutput = model.generate(input_ids, max_length=40, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "\t\t\t\t# get the top 10 tokens and their probabilities\n",
    "\t\t\t\tsoft = softmax(output.scores[0][0])\n",
    "\t\t\t\twords_new3 = torch.topk(soft, 3)\n",
    "\t\t\t\t# print(words[1][0])\n",
    "\t\t\t\tword_new3 = tokenizer.batch_decode(words_new3[1], skip_special_tokens=True)\n",
    "\t\t\t\t# word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "\t\t\t\t# word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "\t\t\t\tfor m in range(3):\n",
    "\t\t\t\t\t# print(t)\n",
    "\t\t\t\t\ttree.create_node(word_new3[m], \"m\"+str(m)+\"l\"+str(l)+\"k\"+str(k)+\"j\"+str(j), parent=\"l\"+str(l)+\"k\"+str(k)+\"j\"+str(j), data=words_new3[0][m])\n",
    "\t\t\t\t\t# text = prompt + \" \" + word[j] + \" \" + word_new[k] + \" \" + word_new2[l] + \" \" + word_new3[m]\n",
    "\t\t\t\t\t# input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\t\t\t\t\t# # output is 30 tokens long and print probability of each token\n",
    "\t\t\t\t\t# output = model.generate(input_ids, max_length=40, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "\t\t\t\t\t# # get the top 10 tokens and their probabilities\n",
    "\t\t\t\t\t# soft = softmax(output.scores[0][0])\n",
    "\t\t\t\t\t# words_new4 = torch.topk(soft, 3)\n",
    "\t\t\t\t\t# # print(words[1][0])\n",
    "\t\t\t\t\t# word_new4 = tokenizer.batch_decode(words_new4[1], skip_special_tokens=True)\n",
    "\t\t\t\t\t# word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "\t\t\t\t\t# word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "# \t\t\t\t\tfor n in range(3):\n",
    "# \t\t\t\t\t\t# print(t)\n",
    "# \t\t\t\t\t\ttree.create_node(word_new4[n], \"n\"+str(n)+\"m\"+str(m)+\"l\"+str(l)+\"k\"+str(k)+\"j\"+str(j), parent=\"m\"+str(m)+\"l\"+str(l)+\"k\"+str(k)+\"j\"+str(j), data=words_new4[0][n])\n",
    "# \t\t\t\t\t\t# text = prompt + \" \" + word[j] + \" \" + word_new[k] + \" \" + word_new2[l] + \" \" + word_new3[m] + \" \" + word_new4[n]\n",
    "# \t\t\t\t\t\t# input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "# \t\t\t\t\t\t# # output is 30 tokens long and print probability of each token\n",
    "# \t\t\t\t\t\t# output = model.generate(input_ids, max_length=50, do_sample=False, top_k=1, top_p=0.1, temperature=0.9, num_return_sequences=1, return_dict_in_generate=True, output_scores=True)\n",
    "# \t\t\t\t\t\t# # get the top 10 tokens and their probabilities\n",
    "# \t\t\t\t\t\t# soft = softmax(output.scores[0][0])\n",
    "# \t\t\t\t\t\t# words_new5 = torch.topk(soft, 3)\n",
    "# \t\t\t\t\t\t# # print(words[1][0])\n",
    "# \t\t\t\t\t\t# word_new5 = tokenizer.batch_decode(words_new5[1], skip_special_tokens=True)\n",
    "# \t\t\t\t\t\t# # word2 = tokenizer.batch_decode(words[1][, skip_special_tokens=True)\n",
    "# \t\t\t\t\t\t# # word3 = tokenizer.batch_decode(words[1][2], skip_special_tokens=True)\n",
    "# \t\t\t\t\t\t# for o in range(3):\n",
    "# \t\t\t\t\t\t# \t# print(t)\n",
    "# \t\t\t\t\t\t# \ttree.create_node(word_new5[o], \"o\"+str(o)+\"n\"+str(n)+\"m\"+str(m)+\"l\"+str(l)+\"k\"+str(k)+\"j\"+str(j), parent=\"n\"+str(n)+\"m\"+str(m)+\"l\"+str(l)+\"k\"+str(k)+\"j\"+str(j), data=words_new5[0][o])\n",
    "# \t\t\t\t\t\t# \ttext = prompt + \" \" + word[j] + \" \" + word_new[k] + \" \" + word_new2[l] + \" \" + word_new3[m] + \" \" + word_new4[n] + \" \" + word_new5[o]\n",
    "# \t\t\t\t\t\t# \tinput_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "# \t\t\t\t\t\t# \t# output is 30 tokens long and print probability of each token\n",
    "# \t\t\t\t\t\t# \toutput = model.generate(input_ids, max_length=60, do_sample=False\n",
    "tree.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eae829583d2fb097fbd757bc95a0f4009c916c9b7dbc1c8b2d2d262108f06b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
