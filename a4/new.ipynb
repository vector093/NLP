{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.1644132286310196, 'start': 0, 'end': 13, 'answer': 'StackOverflow'}\n",
      "--- 5.322897911071777 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# wall time calculation\n",
    "import time\n",
    "start_time = time.time()\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"deepset/roberta-base-squad2\"\n",
    "\n",
    "# Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "    'question': 'Where do developers seek help?',\n",
    "    'context': 'StackOverflow is a popular question-and-answer forum that developers visit to navigate the landscape of software engineering. By both seeking help and providing solutions, StackOverflow has become a pool of vast technical knowledge. As of May 2022, it has garnered over 23 million questions posed by a user base of 18 million, with 70 percent of the questions having at least one answer.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "print(res)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: 'StackOverflow' with score 0.1644132286310196\n"
     ]
    }
   ],
   "source": [
    "print(f\"Answer: '{res['answer']}' with score {res['score']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.00043386517791077495, 'start': 113, 'end': 185, 'answer': 'engineering. By both seeking help and providing solutions, StackOverflow'}\n",
      "--- 1.8367462158203125 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Find a different model that can perform inference at least 25% faster (in wall clock time) than roberta-base-squad2\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "from transformers import pipeline\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Get predictions\n",
    "nlp = pipeline('question-answering', model=model_name, tokenizer=model_name)\n",
    "QA_input = {\n",
    "\t    'question': 'Where do developers seek help?',\n",
    "\t    'context': 'StackOverflow is a popular question-and-answer forum that developers visit to navigate the landscape of software engineering. By both seeking help and providing solutions, StackOverflow has become a pool of vast technical knowledge. As of May 2022, it has garnered over 23 million questions posed by a user base of 18 million, with 70 percent of the questions having at least one answer.'\n",
    "}\n",
    "res = nlp(QA_input)\n",
    "print(res)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 5.28k/5.28k [00:00<00:00, 1.48MB/s]\n",
      "Downloading metadata: 100%|██████████| 2.40k/2.40k [00:00<00:00, 1.17MB/s]\n",
      "Downloading readme: 100%|██████████| 8.02k/8.02k [00:00<00:00, 2.96MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad_v2/squad_v2 to /Users/rohithpudari/.cache/huggingface/datasets/squad_v2/squad_v2/2.0.0/09187c73c1b837c95d9a249cd97c2c3f1cebada06efe667b4427714b27639b1d...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Downloading data: 42.1MB [00:00, 58.6MB/s]\n",
      "\n",
      "Downloading data: 4.37MB [00:00, 61.6MB/s]                  \n",
      "Downloading data files: 100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 514.67it/s]\n",
      "Downloading:  12%|█▏        | 260M/2.24G [16:07<2:02:58, 268kB/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m load_dataset\n\u001b[1;32m      5\u001b[0m \u001b[39m# create datapoint in one line\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m datapoint \u001b[39m=\u001b[39m load_dataset(\u001b[39m'\u001b[39;49m\u001b[39msquad_v2\u001b[39;49m\u001b[39m'\u001b[39;49m, split\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m5\u001b[39m]\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(datapoint)\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/load.py:1742\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, **config_kwargs)\u001b[0m\n\u001b[1;32m   1739\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1741\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1742\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1743\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1744\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1745\u001b[0m     ignore_verifications\u001b[39m=\u001b[39;49mignore_verifications,\n\u001b[1;32m   1746\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1747\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m   1748\u001b[0m )\n\u001b[1;32m   1750\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1752\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1753\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/builder.py:814\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m downloaded_from_gcs:\n\u001b[1;32m    809\u001b[0m     prepare_split_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    810\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfile_format\u001b[39m\u001b[39m\"\u001b[39m: file_format,\n\u001b[1;32m    811\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_shard_size\u001b[39m\u001b[39m\"\u001b[39m: max_shard_size,\n\u001b[1;32m    812\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[1;32m    813\u001b[0m     }\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m    815\u001b[0m         dl_manager\u001b[39m=\u001b[39;49mdl_manager,\n\u001b[1;32m    816\u001b[0m         verify_infos\u001b[39m=\u001b[39;49mverify_infos,\n\u001b[1;32m    817\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs,\n\u001b[1;32m    818\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdownload_and_prepare_kwargs,\n\u001b[1;32m    819\u001b[0m     )\n\u001b[1;32m    820\u001b[0m \u001b[39m# Sync info\u001b[39;00m\n\u001b[1;32m    821\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(split\u001b[39m.\u001b[39mnum_bytes \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39msplits\u001b[39m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/builder.py:1423\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_download_and_prepare\u001b[39m(\u001b[39mself\u001b[39m, dl_manager, verify_infos, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1423\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_download_and_prepare(\n\u001b[1;32m   1424\u001b[0m         dl_manager, verify_infos, check_duplicate_keys\u001b[39m=\u001b[39;49mverify_infos, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_splits_kwargs\n\u001b[1;32m   1425\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/builder.py:905\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    901\u001b[0m split_dict\u001b[39m.\u001b[39madd(split_generator\u001b[39m.\u001b[39msplit_info)\n\u001b[1;32m    903\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    904\u001b[0m     \u001b[39m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m--> 905\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_split(split_generator, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mprepare_split_kwargs)\n\u001b[1;32m    906\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    907\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[1;32m    908\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot find data file. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    909\u001b[0m         \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_download_instructions \u001b[39mor\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    910\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mOriginal error:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    911\u001b[0m         \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    912\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/builder.py:1397\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, max_shard_size)\u001b[0m\n\u001b[1;32m   1387\u001b[0m             shard_id \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1388\u001b[0m             writer \u001b[39m=\u001b[39m writer_class(\n\u001b[1;32m   1389\u001b[0m                 features\u001b[39m=\u001b[39mwriter\u001b[39m.\u001b[39m_features,\n\u001b[1;32m   1390\u001b[0m                 path\u001b[39m=\u001b[39mfpath\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mSSSSS\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mshard_id\u001b[39m:\u001b[39;00m\u001b[39m05d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1395\u001b[0m                 embed_local_files\u001b[39m=\u001b[39membed_local_files,\n\u001b[1;32m   1396\u001b[0m             )\n\u001b[0;32m-> 1397\u001b[0m         example \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo\u001b[39m.\u001b[39;49mfeatures\u001b[39m.\u001b[39;49mencode_example(record) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mfeatures \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m record\n\u001b[1;32m   1398\u001b[0m         writer\u001b[39m.\u001b[39mwrite(example, key)\n\u001b[1;32m   1399\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/features/features.py:1757\u001b[0m, in \u001b[0;36mFeatures.encode_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode_example\u001b[39m(\u001b[39mself\u001b[39m, example):\n\u001b[1;32m   1748\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1749\u001b[0m \u001b[39m    Encode example into a format for Arrow.\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[39m        :obj:`dict[str, Any]`\u001b[39;00m\n\u001b[1;32m   1756\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1757\u001b[0m     example \u001b[39m=\u001b[39m cast_to_python_objects(example)\n\u001b[1;32m   1758\u001b[0m     \u001b[39mreturn\u001b[39;00m encode_nested_example(\u001b[39mself\u001b[39m, example)\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/features/features.py:413\u001b[0m, in \u001b[0;36mcast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcast_to_python_objects\u001b[39m(obj: Any, only_1d_for_numpy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, optimize_list_casting\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    394\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39m    Cast numpy/pytorch/tensorflow/pandas objects to python lists.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \u001b[39m    It works recursively.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[39m        casted_obj: the casted object\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 413\u001b[0m     \u001b[39mreturn\u001b[39;00m _cast_to_python_objects(\n\u001b[1;32m    414\u001b[0m         obj, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    415\u001b[0m     )[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/features/features.py:361\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    359\u001b[0m output \u001b[39m=\u001b[39m {}\n\u001b[1;32m    360\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 361\u001b[0m     casted_v, has_changed_v \u001b[39m=\u001b[39m _cast_to_python_objects(\n\u001b[1;32m    362\u001b[0m         v, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    364\u001b[0m     has_changed \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m has_changed_v\n\u001b[1;32m    365\u001b[0m     output[k] \u001b[39m=\u001b[39m casted_v\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/features/features.py:361\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    359\u001b[0m output \u001b[39m=\u001b[39m {}\n\u001b[1;32m    360\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m obj\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 361\u001b[0m     casted_v, has_changed_v \u001b[39m=\u001b[39m _cast_to_python_objects(\n\u001b[1;32m    362\u001b[0m         v, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    363\u001b[0m     )\n\u001b[1;32m    364\u001b[0m     has_changed \u001b[39m|\u001b[39m\u001b[39m=\u001b[39m has_changed_v\n\u001b[1;32m    365\u001b[0m     output[k] \u001b[39m=\u001b[39m casted_v\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/features/features.py:372\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[39mif\u001b[39;00m _check_non_null_non_empty_recursive(first_elmt):\n\u001b[1;32m    371\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m casted_first_elmt, has_changed_first_elmt \u001b[39m=\u001b[39m _cast_to_python_objects(\n\u001b[1;32m    373\u001b[0m     first_elmt, only_1d_for_numpy\u001b[39m=\u001b[39;49monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39;49moptimize_list_casting\n\u001b[1;32m    374\u001b[0m )\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m has_changed_first_elmt \u001b[39mor\u001b[39;00m \u001b[39mnot\u001b[39;00m optimize_list_casting:\n\u001b[1;32m    376\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    377\u001b[0m         _cast_to_python_objects(\n\u001b[1;32m    378\u001b[0m             elmt, only_1d_for_numpy\u001b[39m=\u001b[39monly_1d_for_numpy, optimize_list_casting\u001b[39m=\u001b[39moptimize_list_casting\n\u001b[1;32m    379\u001b[0m         )[\u001b[39m0\u001b[39m]\n\u001b[1;32m    380\u001b[0m         \u001b[39mfor\u001b[39;00m elmt \u001b[39min\u001b[39;00m obj\n\u001b[1;32m    381\u001b[0m     ], \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/PhD/courses/ece1786hf/a4/venv/lib/python3.10/site-packages/datasets/features/features.py:297\u001b[0m, in \u001b[0;36m_cast_to_python_objects\u001b[0;34m(obj, only_1d_for_numpy, optimize_list_casting)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39mif\u001b[39;00m config\u001b[39m.\u001b[39mPIL_AVAILABLE \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mPIL\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules:\n\u001b[1;32m    295\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mImage\u001b[39;00m\n\u001b[0;32m--> 297\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(obj, np\u001b[39m.\u001b[39;49mndarray):\n\u001b[1;32m    298\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m only_1d_for_numpy \u001b[39mor\u001b[39;00m obj\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    299\u001b[0m         \u001b[39mreturn\u001b[39;00m obj, \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5th datapoint in the training split of squad v2.0\n",
    "# download squad v2.0 training data from huggingface datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# create datapoint in one line\n",
    "datapoint = load_dataset('squad_v2', split='train')[5]\n",
    "print(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eae829583d2fb097fbd757bc95a0f4009c916c9b7dbc1c8b2d2d262108f06b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
